# AI 内容编织应用 - 代码架构设计

本文档概述了“AI 内容编织应用”的代码架构和关键设计决策。

## 1. 总体架构

本应用采用前后端分离的架构：

*   **前端 (Client-Side)**:
    *   使用 **React (Vite)** 构建的单页面应用 (SPA)。
    *   负责用户界面展示、用户输入捕获、与后端 API 的交互。
    *   使用 **Tailwind CSS** 进行样式设计。
*   **后端 (Server-Side)**:
    *   使用 **Python** 和 **FastAPI** 构建的 API 服务。
    *   负责处理用户请求、与 LLM 服务交互、内容处理与编排。
    *   使用 **`uv`** 进行包管理和虚拟环境管理。

## 2. 后端架构 (`backend/`)

FastAPI 应用的核心组件和模块：

*   **`main.py`**:
    *   FastAPI 应用实例的入口。
    *   定义 API 端点 (HTTP 路由)，如 `/api/v1/generate` 和 `/api/v1/llms`。
    *   包含 LLM 提供商的工厂函数 (`get_llm_provider`)，用于根据用户选择动态实例化对应的 LLM 服务。
    *   协调请求处理流程。
*   **`schemas.py`**:
    *   使用 Pydantic 定义所有数据模型，用于请求体验证、响应体序列化以及内部数据结构。
    *   关键模型包括：
        *   `ContentBlockItem` (及具体的 `TextBlock`, `CodeBlock`, `ImageBlock`): 定义用户输入的内容块结构。
        *   `UserInput`: 包含一个 `ContentBlockItem` 列表。
        *   `LLMSelection`: 用户选择的 LLM 提供商和模型。
        *   `GenerationRequest`: `/api/v1/generate` 端点的请求体模型。
        *   `GeneratedContent`: `/api/v1/generate` 端点的响应体模型，包含生成的标题、Markdown、HTML 和建议。
        *   `AvailableLLMsResponse`, `LLMProviderInfo`, `LLMModelInfo`, `ModelCapability`: 用于 `/api/v1/llms` 端点，描述可用的 LLM 及其能力。
*   **`llm_providers/` (包)**:
    *   **`base_llm.py`**: 定义抽象基类 `BaseLLMProvider`，规定了所有 LLM 提供商实现必须遵循的接口（如 `generate_content_from_blocks` 方法）。这确保了可插拔性和扩展性。
    *   **`google_gemini_llm.py`** (及其他未来提供商如 `anthropic_llm.py`, `openai_llm.py`):
        *   特定 LLM 提供商的具体实现，继承自 `BaseLLMProvider`。
        *   每个实现负责：
            1.  根据 `UserInput` 和 `LLMSelection` 构建适合该 LLM 的提示 (prompt)。
            2.  使用相应的 SDK 与 LLM API 进行交互（当前为占位符逻辑）。
            3.  处理 LLM API 的响应。
            4.  将 LLM 的输出（通常是 Markdown）转换为 `GeneratedContent` 对象，包括将 Markdown 转换为 HTML（未来集成 Markdown 库）。
*   **`.venv/`**: 由 `uv` 管理的 Python 虚拟环境，包含项目依赖。

## 3. 前端架构 (`frontend/`)

React (Vite) 应用的核心组件和模块：

*   **`main.jsx`**:
    *   React 应用的入口点，将主 `App` 组件渲染到 DOM。
*   **`App.jsx`**:
    *   应用的主组件和容器。
    *   **状态管理**: 使用 React `useState` 和 `useEffect` Hooks 管理核心应用状态，包括：
        *   `blocks`: 用户输入的内容块数组。
        *   `selectedLlm`: 用户选择的 LLM 提供商和模型。
        *   `isLoading`: 指示 API 请求是否正在进行中。
        *   `generatedArticle`: 存储从后端获取��生成内容。
        *   `isLlmSelectionValid`: 标记 LLM 是否已完整选择。
    *   **核心逻辑**:
        *   函数来添加 (`addBlock`)、更新 (`updateBlock`)、移除 (`removeBlock`) 内容块。
        *   回调函数 (`handleLlmChange`) 更新 `selectedLlm` 状态。
        *   `handleGenerate` 函数：
            *   构建发送到后端 `/api/v1/generate` 的请求体。
            *   (未来) 执行 `fetch` API 调用。
            *   处理 API 响应或错误，并更新 `generatedArticle` 状态。
    *   **组件编排**: 渲染 `LLMSelector` 和 `ContentBlockInput` 组件列表，以及结果展示区域。
*   **`components/` (目录)**:
    *   **`ContentBlockInput.jsx`**:
        *   一个可复用的组件，用于渲染单个内容块的输入界面。
        *   根据传入的 `block.type` (text, code, image) 动态显示不同的表单字段。
        *   处理该块内部数据的变更，并通过回调 (`updateBlock`) 通知父组件 (`App.jsx`)。
        *   包含移除自身块的按钮。
    *   **`LLMSelector.jsx`**:
        *   负责获取和展示可用的 LLM 提供商和模型。
        *   **API 调用**: 在组件挂载时 (`useEffect`) 从后端 `/api/v1/llms` 端点获取 LLM 列表。
        *   **状态管理**: 管理加载状态、错误状态、获取到的 LLM 列表、当前选中的提供商 ID 和模型 ID。
        *   **UI**: 渲染两个下拉选择框（Provider 和 Model）。Model 下拉框的内容根据选中的 Provider 动态更新。
        *   **能力展示与提示**: 显示所选模型的关键能力，并根据 `currentBlocks` (父组件传入的当前用户输入块) 和模型能力（如是否支持图片）给出警告。
        *   通过 `onLlmChange` 回调将用户的选择通知父组件 (`App.jsx`)。
*   **`index.css`**:
    *   包含 Tailwind CSS 的 `@tailwind` 指令，以及任何全局自定义样式。
*   **`tailwind.config.js` 和 `postcss.config.js`**:
    *   Tailwind CSS 和 PostCSS 的配置文件。
*   **`vite.config.js`**:
    *   Vite 构建工具的配置文件，可用于配置开发服务器、代理（如将 `/api` 请求代理到后端）、构建选项等。

## 4. 数据流 (简化版 - 内容生成)

1.  **用户在前端操作**:
    *   通过 `LLMSelector` ��择一个 LLM Provider 和 Model。选择结果更新到 `App.jsx` 的 `selectedLlm` 状态。
    *   通过 `ContentBlockInput` 组件动态添加/编辑/删除内容块。这些变更更新到 `App.jsx` 的 `blocks` 状态。
2.  **触发生成**: 用户点击 "Weave My Article!" 按钮。
3.  **前端 (`App.jsx`)**:
    *   调用 `handleGenerate` 函数。
    *   从 `blocks` 和 `selectedLlm` 状态构造请求体 (`GenerationRequest` 结构)。
    *   (未来) 向后端 `POST /api/v1/generate` 发送异步 `fetch` 请求。
    *   设置 `isLoading` 为 true。
4.  **后端 (`main.py`)**:
    *   `/api/v1/generate` 端点接收请求。
    *   使用 Pydantic 验证请求体。
    *   调用 `get_llm_provider` 工厂函数，根据 `request.llm_selection.provider` 获取相应的 LLM Provider 实例 (如 `GoogleGeminiLLMProvider`)。
5.  **后端 LLM Provider (如 `GoogleGeminiLLMProvider`)**:
    *   调用其 `generate_content_from_blocks` 方法。
    *   (未来) 方法内部：
        *   将 `request.user_input` (包含 `blocks` 列表) 转换为适合所选 LLM 的复杂提示 (prompt)。
        *   使用 LLM SDK 调用外部 LLM API。
        *   获取 LLM 返回的原始内容 (如 Markdown)。
        *   将原始 Markdown 转换为 HTML (使用 Markdown 库)。
        *   提取/生成标题。
        *   组装 `GeneratedContent` 对象。
    *   (当前) 返回占位符 `GeneratedContent` 对象。
6.  **后端 (`main.py`)**:
    *   将 `GeneratedContent` 对象作为 JSON 响应返回给前端。
7.  **前端 (`App.jsx`)**:
    *   接收到 API 响应。
    *   更新 `generatedArticle` 状态。
    *   设置 `isLoading` 为 false。
    *   页面根据新的 `generatedArticle` 状态重新渲染，显示生成的标题、Markdown、HTML 预览和建议。

## 5. 关键设计原则

*   **模块化**: 前后端代码都力求模块化，例如后端的 `llm_providers` 包和前端的 `components` 目录。
*   **类型安全**: 后端使用 Pydantic 进行数据验证和建模，有助于减少运行时错误并提高代码可读性。前端也可以通过 JSDoc 或 TypeScript (未来考虑) 增强类型检查。
*   **可扩展性**:
    *   通过 `BaseLLMProvider` 接口，可以方便地添加对新 LLM 提供商的支持，而无需大量修改核心逻辑。
    *   `/api/v1/llms` 端点使得前端可以动态获取 LLM 列表，后端添加新模型或提供商后，前端可以自动适配。
*   **关注点分离**: 前端负责 UI 和用户交互，后端负责业务逻辑和与外部服务（LLM）的集成。
*   **现代工具链**: 利用 Vite, `uv`, FastAPI, Tailwind CSS 等现代工具提升开发效率和应用性能。

这个设计为应用的初步开发奠定了基础，并为未来的功能扩展提供了良好的框架。
